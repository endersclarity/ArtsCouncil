---
phase: 01-data-pipeline-setup
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - scripts/fetch-hours.py
  - scripts/requirements.txt
  - .env.example
autonomous: false

user_setup:
  - service: google_places_api
    why: "Fetch hours of operation for cultural assets"
    env_vars:
      - name: GOOGLE_PLACES_API_KEY
        source: "Google Cloud Console -> APIs & Services -> Credentials (key already exists from prior work)"
    dashboard_config:
      - task: "Enable Places API (New)"
        location: "Google Cloud Console -> APIs & Services -> Library"
      - task: "Add HTTP referrer restrictions"
        location: "Google Cloud Console -> Credentials -> Edit API key -> Application restrictions"

must_haves:
  truths:
    - "Script successfully fetches Place IDs for assets using Text Search"
    - "Script successfully fetches hours data using Place Details"
    - "Script handles rate limits with 150ms delays and exponential backoff"
    - "Script gracefully handles missing hours data without crashing"
  artifacts:
    - path: "scripts/fetch-hours.py"
      provides: "Google Places API data fetching with rate limiting"
      min_lines: 150
      exports: ["find_place_id", "fetch_hours", "process_venues_with_backoff"]
    - path: "scripts/requirements.txt"
      provides: "Python dependencies"
      contains: "requests"
  key_links:
    - from: "scripts/fetch-hours.py"
      to: "Google Places API"
      via: "requests library with X-Goog-Api-Key header"
      pattern: "requests\\.(get|post).*places\\.googleapis\\.com"
    - from: "scripts/fetch-hours.py"
      to: "website/cultural-map-redesign/data.json"
      via: "read existing data, merge hours fields"
      pattern: "json\\.load.*data\\.json"
---

<objective>
Create the data fetching script that retrieves hours of operation from Google Places API for all 687 cultural assets with proper rate limiting and error handling.

Purpose: Establish the foundation for "Open Now" filtering by obtaining structured hours data for every venue in the cultural map.

Output: Working Python script that can fetch hours data for all assets, respecting API rate limits and handling edge cases gracefully.
</objective>

<execution_context>
@C:\Users\ender\.claude/get-shit-done/workflows/execute-plan.md
@C:\Users\ender\.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/REQUIREMENTS.md
@.planning/phases/01-data-pipeline-setup/01-RESEARCH.md
@website/cultural-map-redesign/data.json
</context>

<tasks>

<task type="auto">
  <name>Create fetch-hours.py script with Google Places API integration</name>
  <files>scripts/fetch-hours.py, scripts/requirements.txt, .env.example</files>
  <action>
Create `scripts/fetch-hours.py` following the research patterns from 01-RESEARCH.md:

**Core functionality:**
1. Load existing data.json (687 assets with structure: `{n: name, a: address, c: city, l: layer, ...}`)
2. For each asset:
   - Use Text Search API to resolve Place ID from name + address + city
   - Use Place Details API to fetch `regularOpeningHours` using Place ID
   - Extract weekday text (e.g., "Monday: 9:00 AM – 5:00 PM") into compact format
   - Add `h` field (hours array) and `pid` field (Place ID) to asset object
3. Write updated data back to data.json (preserve existing fields)

**Rate limiting (DATA-05):**
- 150ms delay between requests with 0-50ms jitter (avoid synchronized bursts)
- Exponential backoff for 429 errors: start at 100ms, double on retry, max 5 retries
- Log progress every 50 venues
- Handle non-rate-limit errors gracefully (log and continue)

**Edge cases:**
- Assets without addresses (trails, monuments) - skip Place ID resolution, set `h: null`
- No Place ID found - log warning, set `pid: null, h: null`
- No hours data - set `h: null` (venue may exist but hours unavailable)
- Malformed API responses - catch JSON errors, continue processing

**Data format:**
- `h` field: Array of strings (e.g., `["Monday: 9:00 AM – 5:00 PM", "Tuesday: 9:00 AM – 5:00 PM", ...]`)
- `pid` field: String Place ID (e.g., `"ChIJN1t_tDeuEmsRUsoyG83frY4"`)

**Configuration:**
- API key from environment variable `GOOGLE_PLACES_API_KEY`
- Use python-dotenv to load from .env file for local testing
- Base URL: `https://places.googleapis.com/v1`
- Required headers: `X-Goog-Api-Key`, `X-Goog-FieldMask`, `Content-Type: application/json`

**Create requirements.txt:**
```
requests>=2.31.0
python-dotenv>=1.0.0
```

**Create .env.example:**
```
GOOGLE_PLACES_API_KEY=your_api_key_here
```

Use the two-step pattern from research (Pattern 1) and rate-limited processing pattern (Pattern 2). Do NOT call the API yet - this task only creates the script.
  </action>
  <verify>
Verify script structure:
```bash
python scripts/fetch-hours.py --help 2>&1 || echo "Script exists"
grep -q "find_place_id" scripts/fetch-hours.py && echo "✓ find_place_id function"
grep -q "fetch_hours" scripts/fetch-hours.py && echo "✓ fetch_hours function"
grep -q "process_venues_with_backoff" scripts/fetch-hours.py && echo "✓ backoff logic"
grep -q "time.sleep" scripts/fetch-hours.py && echo "✓ rate limiting"
```
  </verify>
  <done>
- scripts/fetch-hours.py exists with find_place_id, fetch_hours, and process_venues_with_backoff functions
- Script loads data.json, processes each asset, and writes updated data back
- Rate limiting implemented with 150ms delays and exponential backoff
- Environment variable configuration for API key
- requirements.txt and .env.example created
  </done>
</task>

<task type="checkpoint:human-action" gate="blocking">
  <action>Configure Google Places API key</action>
  <instructions>
**Step 1: Enable Places API (New)**
1. Visit: https://console.cloud.google.com/apis/library
2. Search for "Places API (New)"
3. Click "Enable" (if not already enabled)

**Step 2: Get API key**
The project already has a Google Places API key from prior work (used in google-tools skill for image scraping).

1. Visit: https://console.cloud.google.com/apis/credentials
2. Find existing API key or create new one
3. Copy the key

**Step 3: Add HTTP referrer restrictions (optional for local testing, required for GitHub Actions)**
1. Click on the API key to edit
2. Under "Application restrictions" select "HTTP referrers"
3. Add: `https://culturalmap.vercel.app/*` (production domain)
4. Add: `https://*.vercel.app/*` (preview deployments)
5. Save

**Step 4: Create .env file locally**
```bash
cd C:/Users/ender/.claude/projects/ArtsCouncil
cp .env.example .env
# Edit .env and paste your API key
```

**Step 5: Test API access**
```bash
python scripts/fetch-hours.py --test-api
```

This should print "API key valid" if successful.
  </instructions>
  <resume-signal>Type "api-configured" when Google Places API is enabled and .env file created</resume-signal>
</task>

</tasks>

<verification>
1. Script structure is correct (functions exist, imports present)
2. Rate limiting logic implemented (150ms delays, backoff)
3. Environment variable configuration works
4. API key configured and tested
</verification>

<success_criteria>
- [ ] scripts/fetch-hours.py exists and contains required functions
- [ ] Rate limiting and error handling implemented
- [ ] requirements.txt and .env.example created
- [ ] Google Places API (New) enabled in Cloud Console
- [ ] API key configured in local .env file
- [ ] Test API call succeeds
</success_criteria>

<output>
After completion, create `.planning/phases/01-data-pipeline-setup/01-01-SUMMARY.md`
</output>
