---
phase: 01-data-pipeline-setup
plan: 02
type: execute
wave: 2
depends_on: ["01-01"]
files_modified:
  - website/cultural-map-redesign/data.json
  - .github/workflows/refresh-hours.yml
autonomous: false

must_haves:
  truths:
    - "All 687 assets have been processed (hours fetched or null set)"
    - "data.json contains `h` and `pid` fields for each asset"
    - "GitHub Actions workflow runs on manual trigger"
    - "API key is secured in GitHub Secrets"
  artifacts:
    - path: "website/cultural-map-redesign/data.json"
      provides: "Cultural assets with hours data"
      contains: '"h":'
    - path: ".github/workflows/refresh-hours.yml"
      provides: "Automated hours refresh workflow"
      min_lines: 30
  key_links:
    - from: ".github/workflows/refresh-hours.yml"
      to: "scripts/fetch-hours.py"
      via: "workflow step runs Python script"
      pattern: "python scripts/fetch-hours\\.py"
    - from: ".github/workflows/refresh-hours.yml"
      to: "GitHub Secrets"
      via: "env var from secrets.GOOGLE_PLACES_API_KEY"
      pattern: "\\$\\{\\{ secrets\\.GOOGLE_PLACES_API_KEY \\}\\}"
---

<objective>
Run the fetch-hours script to populate data.json with hours of operation for all 687 assets, then set up GitHub Actions workflow for future automated updates.

Purpose: Complete the data pipeline by executing the initial hours fetch and establishing the automation infrastructure for future updates.

Output: data.json file with `h` and `pid` fields populated, GitHub Actions workflow configured and tested.
</objective>

<execution_context>
@C:\Users\ender\.claude/get-shit-done/workflows/execute-plan.md
@C:\Users\ender\.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/REQUIREMENTS.md
@.planning/phases/01-data-pipeline-setup/01-RESEARCH.md
@.planning/phases/01-data-pipeline-setup/01-01-SUMMARY.md
</context>

<tasks>

<task type="auto">
  <name>Run fetch-hours.py to populate data.json with hours data</name>
  <files>website/cultural-map-redesign/data.json</files>
  <action>
Execute the fetch-hours script to fetch hours for all 687 assets (DATA-01, DATA-02):

```bash
cd C:/Users/ender/.claude/projects/ArtsCouncil
python scripts/fetch-hours.py
```

**Expected behavior:**
- Script processes all 687 assets from data.json
- Progress logged every 50 venues
- Total execution time: ~3-4 minutes (150ms × 687 assets × 2 API calls = ~3.4 min + backoff)
- Rate limiting respected (150ms delays between requests)
- Exponential backoff for 429 errors
- Graceful handling of missing data (null values for assets without hours)

**Output verification:**
- data.json file modified (check git diff)
- New fields added: `h` (hours array or null), `pid` (Place ID string or null)
- File size increased but remains reasonable (<500KB)
- No error messages indicating script failures

**Error handling:**
If script fails partway through:
1. Check error message (rate limit? API quota? malformed response?)
2. Script should be idempotent - safe to re-run (skips assets that already have `pid`)
3. Retry with exponential backoff if rate limited

**Cost estimate validation:**
After completion, check Google Cloud Console -> APIs & Services -> Dashboard to verify actual API usage:
- Expected: ~1,374 requests (687 Text Search + 687 Place Details)
- Expected cost: ~$0 (within 1,000 free tier) or ~$13.60 if exceeding free tier
- Confirm cost aligns with research estimates (~$50/year for weekly refreshes)

Do NOT commit the updated data.json yet - wait for verification checkpoint.
  </action>
  <verify>
Check data.json modifications:
```bash
# Verify hours fields added
grep -c '"h":' website/cultural-map-redesign/data.json
# Should output: 687

# Verify Place IDs added
grep -c '"pid":' website/cultural-map-redesign/data.json
# Should output: 687

# Check file size (should be <500KB)
ls -lh website/cultural-map-redesign/data.json

# View sample asset with hours
head -100 website/cultural-map-redesign/data.json | grep -A5 '"h":'
```
  </verify>
  <done>
- Script execution completed successfully
- data.json contains `h` and `pid` fields for all 687 assets
- Hours data formatted as array of strings (e.g., ["Monday: 9:00 AM – 5:00 PM", ...])
- Assets without hours have `h: null` and `pid: null`
- File size remains reasonable (<500KB)
  </done>
</task>

<task type="checkpoint:human-verify" gate="blocking">
  <what-built>Initial hours data fetch - data.json populated with `h` and `pid` fields for all 687 cultural assets</what-built>
  <how-to-verify>
**Step 1: Review data.json changes**
```bash
git diff website/cultural-map-redesign/data.json | head -200
```

Look for:
- `"h":` fields with hours arrays (e.g., `["Monday: 9:00 AM – 5:00 PM", "Tuesday: 9:00 AM – 5:00 PM", ...]`)
- `"pid":` fields with Place IDs (e.g., `"ChIJN1t_tDeuEmsRUsoyG83frY4"`)
- `null` values for assets without hours/IDs (trails, monuments)

**Step 2: Spot-check known venues**
Pick 3-5 venues you know have regular hours (museums, galleries) and verify:
1. Hours data looks accurate (matches Google Maps)
2. Format is readable (day names, AM/PM times)
3. No obvious errors (garbled text, wrong timezone)

**Step 3: Check coverage**
```bash
# Count venues with hours
grep -o '"h":\[' website/cultural-map-redesign/data.json | wc -l
# Count venues without hours
grep -o '"h":null' website/cultural-map-redesign/data.json | wc -l
```

Expected:
- 400-500 venues with hours (museums, galleries, restaurants, venues)
- 200-300 venues without hours (trails, monuments, outdoor sites)

**Step 4: Verify API costs**
Visit Google Cloud Console -> APIs & Services -> Dashboard:
- Check actual API usage (should be ~1,374 requests)
- Verify cost is within expectations (~$0-$15)

**If hours data looks good, type "approved". If issues found, describe them.**
  </how-to-verify>
  <resume-signal>Type "approved" if data looks correct, or describe issues</resume-signal>
</task>

<task type="auto">
  <name>Set up GitHub Actions workflow for automated hours refresh</name>
  <files>.github/workflows/refresh-hours.yml</files>
  <action>
Create GitHub Actions workflow for weekly hours refresh (DATA-03, DATA-04):

**Create `.github/workflows/refresh-hours.yml`:**

```yaml
name: Refresh Hours Data

on:
  schedule:
    # Every Monday at 8 AM UTC (midnight Pacific)
    - cron: '0 8 * * 1'
  workflow_dispatch:  # Manual trigger for testing

jobs:
  refresh-hours:
    runs-on: ubuntu-latest
    permissions:
      contents: write  # Required for git-auto-commit-action

    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'

      - name: Install dependencies
        run: |
          pip install -r scripts/requirements.txt

      - name: Fetch hours data
        env:
          GOOGLE_PLACES_API_KEY: ${{ secrets.GOOGLE_PLACES_API_KEY }}
        run: |
          python scripts/fetch-hours.py

      - name: Commit updated data
        uses: stefanzweifel/git-auto-commit-action@v5
        with:
          commit_message: "chore(data): refresh hours of operation"
          file_pattern: "website/cultural-map-redesign/data.json"
          commit_user_name: "github-actions[bot]"
          commit_user_email: "github-actions[bot]@users.noreply.github.com"

      - name: Report results
        if: always()
        run: |
          echo "Hours refresh workflow completed"
          git log -1 --oneline
```

**Pattern from research:** Uses Pattern 3 (GitHub Actions Scheduled Data Refresh) with stefanzweifel/git-auto-commit-action@v5 for automatic commits.

**Schedule:** Weekly refresh on Monday mornings (hours don't change frequently, daily refresh unnecessary per PROJECT.md decisions).

**Workflow dispatch:** Allows manual triggering for testing and emergency updates.

After creating workflow, add API key to GitHub Secrets:

1. Visit: https://github.com/endersclarity/ArtsCouncil/settings/secrets/actions
2. Click "New repository secret"
3. Name: `GOOGLE_PLACES_API_KEY`
4. Value: [paste API key from .env file]
5. Click "Add secret"

**Test the workflow:**
```bash
# Commit and push workflow file
git add .github/workflows/refresh-hours.yml
git commit -m "ci: add hours refresh GitHub Actions workflow"
git push

# Trigger workflow manually via GitHub UI or CLI
gh workflow run "Refresh Hours Data"

# Check workflow status
gh run list --workflow="Refresh Hours Data"
```

Wait for workflow to complete (check GitHub Actions tab) and verify it successfully updates data.json.
  </action>
  <verify>
Verify workflow setup:
```bash
# Check workflow file exists
ls .github/workflows/refresh-hours.yml

# Validate YAML syntax
python -c "import yaml; yaml.safe_load(open('.github/workflows/refresh-hours.yml'))" && echo "✓ Valid YAML"

# Check workflow is recognized by GitHub
gh workflow list | grep "Refresh Hours"

# Trigger test run
gh workflow run "Refresh Hours Data"

# Check status after ~5 minutes
gh run list --workflow="Refresh Hours Data" --limit 1
```
  </verify>
  <done>
- .github/workflows/refresh-hours.yml created with weekly cron schedule
- Workflow includes all steps: checkout, Python setup, dependencies, fetch script, auto-commit
- API key added to GitHub Secrets (GOOGLE_PLACES_API_KEY)
- Manual workflow trigger tested successfully
- Workflow completes without errors and commits updated data.json
  </done>
</task>

<task type="checkpoint:human-verify" gate="blocking">
  <what-built>GitHub Actions workflow for automated hours refresh - runs weekly and on manual trigger</what-built>
  <how-to-verify>
**Step 1: Check GitHub Secrets**
Visit: https://github.com/endersclarity/ArtsCouncil/settings/secrets/actions

Confirm:
- `GOOGLE_PLACES_API_KEY` secret exists
- Created date is recent

**Step 2: Review workflow file**
```bash
cat .github/workflows/refresh-hours.yml
```

Verify:
- Cron schedule: `0 8 * * 1` (Monday 8 AM UTC)
- Manual trigger: `workflow_dispatch` present
- Python version: 3.11
- Script path: `python scripts/fetch-hours.py`
- Auto-commit action: stefanzweifel/git-auto-commit-action@v5

**Step 3: Check workflow execution**
Visit: https://github.com/endersclarity/ArtsCouncil/actions

Look for:
- "Refresh Hours Data" workflow in list
- Most recent run shows green checkmark (success)
- Run time: ~3-5 minutes
- Commit created by github-actions[bot]

**Step 4: Test manual trigger**
```bash
gh workflow run "Refresh Hours Data"
# Wait 5 minutes
gh run list --workflow="Refresh Hours Data" --limit 1
```

Should show: completed successfully, no errors

**If workflow runs successfully, type "approved". If issues, describe them.**
  </how-to-verify>
  <resume-signal>Type "approved" if workflow works correctly, or describe issues</resume-signal>
</task>

</tasks>

<verification>
1. data.json contains hours data (`h` field) and Place IDs (`pid` field) for all assets
2. Hours data quality spot-checked for known venues
3. GitHub Actions workflow created and tested
4. API key secured in GitHub Secrets
5. Manual workflow trigger succeeds
</verification>

<success_criteria>
- [ ] fetch-hours.py executed successfully
- [ ] data.json updated with `h` and `pid` fields (687 assets)
- [ ] Hours data spot-checked for accuracy
- [ ] GitHub Actions workflow file created
- [ ] API key added to GitHub Secrets
- [ ] Workflow manual trigger tested successfully
- [ ] No errors in workflow execution logs
</success_criteria>

<output>
After completion, create `.planning/phases/01-data-pipeline-setup/01-02-SUMMARY.md`
</output>
