---
phase: 02-tier-2-events
plan: 02
type: execute
wave: 2
depends_on: ["02-01"]
files_modified:
  - scripts/events/merge_events.py
  - scripts/events/family_keywords.json
  - .github/workflows/refresh-events.yml
autonomous: true

must_haves:
  truths:
    - "Running merge_events.py combines Trumba + LibCal + CivicEngage events into a single deduplicated list"
    - "Cross-source duplicates (same title + date + venue) appear only once, with the Trumba version preferred"
    - "Family-friendly events are tagged with is_family: true using keyword classification"
    - "The GitHub Actions workflow runs daily and commits events-merged.json"
    - "events-merged.json includes generated_at timestamp and source_counts summary"
    - "build_event_index.py can consume the merged output without modification"
  artifacts:
    - path: "scripts/events/merge_events.py"
      provides: "Event merge, dedup, and family classification pipeline"
      min_lines: 120
    - path: "scripts/events/family_keywords.json"
      provides: "Configurable family keyword patterns"
      contains: "storytime"
    - path: ".github/workflows/refresh-events.yml"
      provides: "Daily cron workflow for event aggregation"
      contains: "cron"
  key_links:
    - from: "scripts/events/merge_events.py"
      to: "website/cultural-map-redesign/events-merged.json"
      via: "file output"
      pattern: "events-merged\\.json"
    - from: ".github/workflows/refresh-events.yml"
      to: "scripts/events/ingest_libcal_ical.py"
      via: "workflow step"
      pattern: "ingest_libcal_ical"
    - from: ".github/workflows/refresh-events.yml"
      to: "scripts/events/merge_events.py"
      via: "workflow step"
      pattern: "merge_events"
    - from: ".github/workflows/refresh-events.yml"
      to: "scripts/events/build_event_index.py"
      via: "workflow step with --events-file flag"
      pattern: "build_event_index.*events-merged-flat"
---

<objective>
Create the merge/dedup/classify pipeline that combines all three event sources into `events-merged.json`, and the GitHub Actions workflow that runs the full pipeline daily.

Purpose: This is the critical data pipeline that turns three separate event feeds into a single, deduplicated, classified dataset. Without it, the client has nothing new to display.
Output: A merge script, family keyword config, and a daily cron workflow.
</objective>

<execution_context>
@C:/Users/ender/.claude/get-shit-done/workflows/execute-plan.md
@C:/Users/ender/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/phases/02-tier-2-events/02-RESEARCH.md
@.planning/phases/02-tier-2-events/02-01-SUMMARY.md
@scripts/events/ingest_trumba_rss.py
@scripts/events/build_event_index.py
@.github/workflows/refresh-hours.yml
</context>

<tasks>

<task type="auto">
  <name>Task 1: Merge/dedup/classify script with family keywords config</name>
  <files>scripts/events/merge_events.py, scripts/events/family_keywords.json</files>
  <action>
Create `scripts/events/family_keywords.json`:
```json
{
  "positive_patterns": [
    "\\bstorytime\\b", "\\bstory\\s*time\\b",
    "\\bfor\\s+kids\\b", "\\bfor\\s+children\\b", "\\bfor\\s+families\\b",
    "\\bfamily[\\s-]friendly\\b", "\\ball\\s+ages\\b",
    "\\bages?\\s+\\d+[\\s-]+\\d+\\b",
    "\\bchildren'?s\\s+(program|event|craft|activity)\\b",
    "\\bkids?\\s+(program|event|craft|activity|club)\\b",
    "\\bteen\\b", "\\btween\\b", "\\bjunior\\b",
    "\\blego\\b", "\\bcraft\\s*time\\b", "\\bplay\\s*group\\b",
    "\\bstay\\s*&?\\s*play\\b"
  ],
  "negative_patterns": [
    "\\bchildren\\s+of\\b",
    "\\bfor\\s+adults\\s+only\\b",
    "\\b21\\+\\b", "\\b18\\+\\b",
    "\\bbar\\b.*\\bnight\\b"
  ]
}
```

Create `scripts/events/merge_events.py`:
- argparse CLI with:
  - `--trumba-file` (default: `website/cultural-map-redesign/events.json`)
  - `--libcal-file` (default: `website/cultural-map-redesign/events-libcal.json`)
  - `--civicengage-file` (default: `website/cultural-map-redesign/events-civicengage.json`)
  - `--output-file` (default: `website/cultural-map-redesign/events-merged.json`)
  - `--flat-output-file` (default: `website/cultural-map-redesign/events-merged-flat.json`)
  - `--family-keywords` (default: `scripts/events/family_keywords.json`)
  - `--dedup-threshold` (default: 85, the rapidfuzz score threshold)
  - `--venue-threshold` (default: 70, the rapidfuzz venue match threshold)

Pipeline steps:
1. **Load** all three source files. Each is `{"generated_at": "...", "events": [...]}`. If a file is missing or unparseable, log warning and skip (graceful degradation -- Trumba alone is valid). Special case: Trumba's `events.json` is a bare JSON array (no wrapper), so detect both formats: if loaded JSON is a list, use it directly; if it's a dict with `events` key, extract the array.
2. **Normalize** all events to canonical schema. Trumba events from existing `events.json` may lack `source_label` -- add `"source_label": "Nevada County Arts Council"` and `"source_type": "feed"` if missing.
3. **Dedup** cross-source using `rapidfuzz.fuzz.token_sort_ratio` on BOTH title AND venue:
   - Group events by date (`start_iso[:10]`)
   - Within each date group, compare all pairs across different sources
   - Title normalization: lowercase, strip leading "the/a/an", collapse whitespace, strip punctuation
   - Venue normalization: lowercase, strip common suffixes ("theater"/"theatre", "center"/"centre", "hall"), collapse whitespace, strip punctuation
   - **Match criteria (BOTH must pass):**
     - Title similarity >= `--dedup-threshold` (default 85) using `rapidfuzz.fuzz.token_sort_ratio`
     - Venue similarity >= `--venue-threshold` (default 70) using `rapidfuzz.fuzz.token_sort_ratio` on normalized venue names. If EITHER event has empty/missing venue_name, skip venue check (treat as potential match on title alone -- better to dedup aggressively than miss).
   - When duplicate found: keep Trumba > LibCal > CivicEngage (priority order). Merge `source_labels` list from all matched events onto the winner.
4. **Classify family** events using `family_keywords.json`:
   - Concatenate title + description + tags into search text
   - Check negative patterns first (any match -> `is_family: false`)
   - Check positive patterns (any match -> `is_family: true`)
   - Default: `is_family: false`
5. **Output** two files:

   `events-merged.json` (wrapped format for the client):
   ```json
   {
     "generated_at": "2026-02-14T08:00:00-08:00",
     "source_counts": {"trumba": 155, "libcal": 42, "civicengage": 3},
     "dedup_removed": 5,
     "family_tagged": 18,
     "events": [...]
   }
   ```

   `events-merged-flat.json` (bare array for `build_event_index.py` compatibility):
   This is just the `events` array written as a top-level JSON array. `build_event_index.py` expects `isinstance(events, list)` to be true (see line 375 of that script). Writing a separate flat file avoids modifying `build_event_index.py` which is shared with the existing Trumba pipeline.

   Both files have events sorted by `start_iso` then `event_id`.

Print summary to stderr: source counts, dedup removals (with venue match details), family tags, total output count.
  </action>
  <verify>
Run the full pipeline locally:
```bash
python scripts/events/ingest_trumba_rss.py
python scripts/events/ingest_libcal_ical.py
python scripts/events/ingest_civicengage_rss.py
python scripts/events/merge_events.py --output-file /tmp/events-merged.json --flat-output-file /tmp/events-merged-flat.json
python -c "
import json
d = json.load(open('/tmp/events-merged.json'))
print(f'Total: {len(d[\"events\"])}, Sources: {d[\"source_counts\"]}, Deduped: {d[\"dedup_removed\"]}, Family: {d[\"family_tagged\"]}')
# Verify dedup worked: no two events with same date + very similar title + venue
from collections import Counter
dates = Counter(e['start_iso'][:10] for e in d['events'])
print(f'Events across {len(dates)} dates')
# Verify family tags exist
family = [e for e in d['events'] if e.get('is_family')]
print(f'Family events: {len(family)}')
# Verify flat file is a bare array compatible with build_event_index.py
flat = json.load(open('/tmp/events-merged-flat.json'))
assert isinstance(flat, list), f'Flat file must be a list, got {type(flat)}'
assert len(flat) == len(d['events']), 'Flat and wrapped must have same event count'
print(f'Flat file: {len(flat)} events (bare array, build_event_index.py compatible)')
"
```
  </verify>
  <done>Merge script combines all sources, removes cross-source duplicates using title+venue fuzzy matching, classifies family events, and outputs both events-merged.json (wrapped, for client) and events-merged-flat.json (bare array, for build_event_index.py). Total event count exceeds Trumba-only count.</done>
</task>

<task type="auto">
  <name>Task 2: GitHub Actions daily cron workflow</name>
  <files>.github/workflows/refresh-events.yml</files>
  <action>
Create `.github/workflows/refresh-events.yml` following the pattern of `refresh-hours.yml`:

```yaml
name: Refresh Events Data

on:
  schedule:
    - cron: "0 7 * * *"  # Daily at 7am UTC (midnight Pacific)
  workflow_dispatch:

permissions:
  contents: write

jobs:
  refresh-events:
    runs-on: ubuntu-latest
    timeout-minutes: 15

    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.12"

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r scripts/requirements.txt

      - name: Ingest Trumba RSS
        run: python scripts/events/ingest_trumba_rss.py

      - name: Ingest LibCal iCal
        run: python scripts/events/ingest_libcal_ical.py

      - name: Ingest CivicEngage RSS
        run: python scripts/events/ingest_civicengage_rss.py
        continue-on-error: true  # CivicEngage is bonus, don't fail pipeline

      - name: Merge and deduplicate events
        run: python scripts/events/merge_events.py

      - name: Rebuild venue index from merged events
        run: python scripts/events/build_event_index.py --events-file website/cultural-map-redesign/events-merged-flat.json

      - name: Commit updated event data
        uses: stefanzweifel/git-auto-commit-action@v5
        with:
          commit_message: "chore(data): refresh events from Trumba + LibCal + CivicEngage"
          file_pattern: "website/cultural-map-redesign/events*.json scripts/events/reports/*.json"
          commit_user_name: "github-actions[bot]"
          commit_user_email: "github-actions[bot]@users.noreply.github.com"

      - name: Report completion
        if: always()
        run: |
          echo "Refresh Events workflow completed"
          python -c "import json; d=json.load(open('website/cultural-map-redesign/events-merged.json')); print(f'Total: {len(d[\"events\"])}, Sources: {d[\"source_counts\"]}')" || true
```

Key points:
- `build_event_index.py` is called with `--events-file website/cultural-map-redesign/events-merged-flat.json` so it reads the bare-array output from merge_events.py. This is critical: `build_event_index.py` (line 375) does `if not isinstance(events, list): fail(...)` -- it expects a raw JSON array, NOT the `{"generated_at": ..., "events": [...]}` wrapper format. The `events-merged-flat.json` file is exactly this bare array.
- Removed `validate_events.py` step (that script does not exist and is not in scope for this phase).
- Daily cron (not weekly) because events change more frequently than hours.
- `continue-on-error: true` on CivicEngage step (bonus source).
- Commits all events*.json files and the match report.
  </action>
  <verify>Validate the YAML syntax: `python -c "import yaml; yaml.safe_load(open('.github/workflows/refresh-events.yml'))"` (install PyYAML if needed, or just verify the file is valid YAML by inspection). Verify all script paths referenced in the workflow exist on disk. Specifically confirm: the `build_event_index.py` step uses `--events-file` pointing to `events-merged-flat.json`.</verify>
  <done>GitHub Actions workflow file exists, references all pipeline scripts in correct order, uses daily cron, passes --events-file events-merged-flat.json to build_event_index.py for format compatibility, and commits output files. CivicEngage step has continue-on-error for graceful degradation.</done>
</task>

</tasks>

<verification>
1. Full pipeline runs locally end-to-end: Trumba + LibCal + CivicEngage -> merge -> events-merged.json + events-merged-flat.json
2. events-merged.json has more events than events.json alone
3. No duplicate events (same date + similar title + similar venue) in merged output
4. Dedup uses BOTH title similarity (>=85) AND venue similarity (>=70) for cross-source matching
5. events-merged-flat.json is a bare JSON array that `build_event_index.py` can consume directly
6. Family events are tagged (at least some LibCal storytimes should be is_family: true)
7. GitHub Actions YAML is valid, references correct file paths, and passes --events-file to build_event_index.py
8. Missing source files don't crash the merge script (graceful fallback)
</verification>

<success_criteria>
The merge pipeline produces a deduplicated, family-classified events-merged.json that the client (Plan 03) can fetch, plus a events-merged-flat.json that build_event_index.py can consume without modification. The GitHub Actions workflow automates daily refresh. If any single source fails, the pipeline still produces valid output from the remaining sources.
</success_criteria>

<output>
After completion, create `.planning/phases/02-tier-2-events/02-02-SUMMARY.md`
</output>
